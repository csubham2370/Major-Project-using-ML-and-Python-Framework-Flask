{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/csubham2370/Major-Project-using-ML-and-Python-Framework-Flask/blob/main/Crop_Recommendation_System2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r58EWITjzW5M"
      },
      "source": [
        "By combining these imports, you're setting up your Python environment for data analysis, visualization, and machine learning tasks while also configuring it to handle warnings in a specific way."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "kd_SRaff2ZaL"
      },
      "outputs": [],
      "source": [
        "# Importing libraries\n",
        "\n",
        "from __future__ import print_function # This line imports the 'print_funcation' feature from the '__further__'model.it ensures that the script will use the print funcation from python 3.x even if it's running in a python 2.x environment.\n",
        "\n",
        "import pandas as pd # This line imports the pandas library and assigns it the alias 'pd'. Pandas is a powerful data manipulation and analysis library for Python.\n",
        "\n",
        "import numpy as np # This line imports the NumPy library and assigns it the alias 'np'. Numpy ia a fundamental package for numerical computing in Python, provading support for large multi-dimensional arrays and matrices, along with a collection of mathematical funcation to operate on these array.\n",
        "\n",
        "import matplotlib.pyplot as plt # This line imports the pyplot module from the matplotlib library and assigns it the alias 'plt. Matplotlib is a plotting library for Python, and pyplot provides a MATLAB-like interface for creating and customizing plots.\n",
        "\n",
        "import seaborn as sns # This line imports the seaborn library and assigns it the alias 'sns'. Seaborn is a statistical data visualization library based on matplotlib. It provides a high-level interface for drawing attractive and informative statistical graphics.\n",
        "\n",
        "from sklearn.metrics import classification_report # This line imports the 'classification_report' funcation from the 'metrics' module of the scikit-learn library. This funcation generates a comprehensive report for evaluating the performance of a classification model.\n",
        "\n",
        "from sklearn import metrics # This line imports the 'metrics' module from scikit-learn. scikit-learn is a machine learning library for Python that provides simple and efficent tools for data mining and data analysis. The 'metrics' module contains various metrics for evaluating the performance of machine learning models.\n",
        "\n",
        "from sklearn import tree # This line imports the tree module from scikit-learn. It provides tools for working with decision trees, which are a type of supervised learning algorithm used for classification and regression tasks.\n",
        "\n",
        "import warnings # This line imports the warnings module, which provides a mechanism to control the behavior of warnings in Python code.\n",
        "\n",
        "warnings.filterwarnings('ignore') # This line sets the warning filter to ignore all warnings. This is useful for suppressing warning messages that might not be relevant or useful for the current analysis. However, it's generally recommended to handle specific warnings appropriately rather than ignoring them altogether."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkqR8SnA2k8Z"
      },
      "source": [
        "# Upload a file on Google Colab\n",
        "This code allows the user to upload a file from their local system to a Google Colab notebook environment, and the uploaded file(s) information is stored in the uploaded variable for further processing within the notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RGTF-GWp2osw",
        "outputId": "21fa7f18-2c7e-4f7a-99ed-b204d3436341"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive # Import the 'drive' module from the 'google.colab' package. This module is specifically designed for interacting with Google Drive in the Colab environment.\n",
        "\n",
        "drive.mount('/content/gdrive') # Mount your Google Drive to the specified directory (/content/gdrive) within the Colab virtual machine. This allows you to access your Google Drive files directly in Colab notebooks as if they were local files. This line will prompt for authorization to access your Google Drive, and you'll need to follow the link, authenticate, and copy the authorization code back into Colab to complete the mounting process."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "teaDJu314J71"
      },
      "source": [
        "# Importing the Data\n",
        "This code reads the contents of a CSV file named 'modified_crop.csv' using Pandas and stores it in a DataFrame named crop for further processing and analysis in Python.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "vlv0JUCU4RgF",
        "outputId": "56b6da42-ab33-4640-f0b9-d6e38786f5e6"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/gdrive/My Drive/modified_crop.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-4afa6cfa37c4>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcrop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/gdrive/My Drive/modified_crop.csv'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#This line reads a CSV file named modified_crop.csv located in the My Drive directory of your mounted Google Drive. The function pd.read_csv() from the pandas library is used to load the CSV file into a pandas DataFrame named crop. This allows you to work with the data in the CSV file using pandas' powerful data manipulation tools within your Colab notebook.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mcrop\u001b[0m \u001b[0;31m# This line, when placed at the end of a cell in a Jupyter notebook or a Google Colab notebook, displays the DataFrame crop. It's a simple way to output and view the DataFrame contents directly in the notebook. If this line is part of a script in a different environment, it won't actually display the DataFrame unless wrapped in a print function or similar construct. In a notebook, it helps quickly visualize the data you've loaded.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    910\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    911\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 912\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    575\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 577\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    578\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1659\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1660\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1661\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1662\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1663\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    857\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    860\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/gdrive/My Drive/modified_crop.csv'"
          ]
        }
      ],
      "source": [
        "crop = pd.read_csv('/content/gdrive/My Drive/modified_crop.csv') #This line reads a CSV file named modified_crop.csv located in the My Drive directory of your mounted Google Drive. The function pd.read_csv() from the pandas library is used to load the CSV file into a pandas DataFrame named crop. This allows you to work with the data in the CSV file using pandas' powerful data manipulation tools within your Colab notebook.\n",
        "\n",
        "crop # This line, when placed at the end of a cell in a Jupyter notebook or a Google Colab notebook, displays the DataFrame crop. It's a simple way to output and view the DataFrame contents directly in the notebook. If this line is part of a script in a different environment, it won't actually display the DataFrame unless wrapped in a print function or similar construct. In a notebook, it helps quickly visualize the data you've loaded."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5JRE86l040SU"
      },
      "source": [
        "# crop.shape\n",
        "This code is use for quickly check the size of your DataFrame, which is especially useful when dealing with large datasets or when you need to understand the structure of the data you're working with.\n",
        "\n",
        "For example, if you run crop.shape and it returns (100, 5), it means that the DataFrame crop has 100 rows and 5 columns. and the number of elements is 500."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qrp_ykhj44p2"
      },
      "outputs": [],
      "source": [
        "  crop.shape # This line returns a tuple containing the number of rows and columns in the DataFrame crop.\n",
        "             # The first element of the tuple represents the number of rows, and the second element represents the number of columns.\n",
        "             # It's a quick way to ascertain the size or dimensions of the DataFrame, providing insight into the structure and size of the dataset loaded from the CSV file.\n",
        "\n",
        "             #Tuples are immutable in nature. Thus, we cannot make changes after creating it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "efO8G09P5vIx"
      },
      "source": [
        "# crop.info()\n",
        "By using crop.info(), you can quickly get an overview of the DataFrame, including its size, data types, and missing values, which is useful for initial data exploration and understanding the dataset's characteristics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MTs6jIk752QQ"
      },
      "outputs": [],
      "source": [
        "crop.info() # This line generates a summary of the DataFrame crop, including information about its index dtype, column dtypes, non-null values,\n",
        "            # and memory usage. It's a useful method to quickly inspect the data types of each column, the presence of missing values, and an estimate of the memory usage of the DataFrame.\n",
        "            # This summary helps in understanding the composition of the dataset and identifying potential issues or areas for further exploration or cleaning.\n",
        "            #Strings are objects in Python. immutable."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZnK_kVD26fyi"
      },
      "source": [
        "# crop.head()\n",
        "The purpose of using head() is to quickly inspect the structure and content of the DataFrame. It's often used as an initial step in data analysis to get a sense of what the data looks like before performing further operations. By examining the first few rows, you can check the column names, data types, and example values in the DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cCGGZ3Ai6mJ8"
      },
      "outputs": [],
      "source": [
        "crop.head() # This line retrieves the first five rows of the DataFrame crop and displays them.\n",
        "            # The head() method is commonly used to preview the beginning of a DataFrame, providing a quick overview of the data's format,\n",
        "            # column names, and some sample values. By default, it returns the first five rows, but you can specify a different number of rows to display by passing an argument to the head() method (e.g., crop.head(10),\n",
        "            # to display the first ten rows). It's particularly useful for understanding the structure of the dataset and identifying any potential issues or patterns at the outset of data analysis.\n",
        "            # If we passing the 0 as a parameter then only columns are print."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qhohHUS563AO"
      },
      "source": [
        "# tail()\n",
        "The purpose of using tail() is to quickly inspect the end of the DataFrame. It's often used to check for patterns or trends in the data, especially if the data is ordered chronologically or by some other criteria. By examining the last few rows, you can see the most recent data entries and verify that the DataFrame has been properly loaded or processed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t2nJnWDI65hr"
      },
      "outputs": [],
      "source": [
        "crop.tail() # This line retrieves the last five rows of the DataFrame crop and displays them.\n",
        "            # The tail() method is frequently used to preview the end of a DataFrame, offering a rapid overview of the data's format, column names, and some example values.\n",
        "            # By default, it returns the last five rows, but you can specify a different number of rows to display by passing an argument to the tail() method (e.g., crop.tail(10) to display the last ten rows).\n",
        "            # It's especially useful for understanding the structure of the dataset and identifying any potential issues or patterns towards the conclusion of data analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aJxt3O217UNV"
      },
      "source": [
        "# crop.isnull().sum()\n",
        "crop.isnull().sum() gives you a Series where each entry represents the number of missing values in the corresponding column of the crop DataFrame. This information is useful for identifying and handling missing data in your dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Ia1Se6u7Xi-"
      },
      "outputs": [],
      "source": [
        "crop.isnull().sum() # This line applies the isnull() method to the DataFrame crop, which returns a DataFrame of the same shape as crop with True for missing values (NaNs) and False for non-missing values.\n",
        "                    # Then, the sum() method is applied to this resulting DataFrame, which calculates the sum of True values (i.e., the number of missing values) for each column.\n",
        "                    # The result is a Series where the index represents column names, and the values represent the total number of missing values in each column.\n",
        "                    # This summary is useful for identifying columns with missing data and assessing the extent of missingness in the dataset, which informs decisions about data cleaning or imputation strategies.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJRVRtJvIHA7"
      },
      "source": [
        "![image.png](data:image/png;base64,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)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FOR9znd3IOnA"
      },
      "outputs": [],
      "source": [
        "sns.heatmap(crop.isnull(),yticklabels=False,cbar=True,cmap='viridis') # This line creates a heatmap using Seaborn's heatmap() function. The input to this function is the DataFrame crop with the isnull() method applied,\n",
        "                                                                      # which returns a DataFrame of the same shape with True for missing values and False for non-missing values.\n",
        "                                                                      # This DataFrame is used as the data for the heatmap, where missing values are represented as True (1) and non-missing values as False (0).\n",
        "                                                                      # The yticklabels=False parameter removes the y-axis tick labels, making the heatmap cleaner.\n",
        "                                                                      # The cbar=True parameter adds a color bar to the side of the heatmap, indicating the scale of the colors.\n",
        "                                                                      # The cmap='viridis' parameter sets the color map to 'viridis', which is a perceptually uniform color map that helps in distinguishing different levels,\n",
        "                                                                      # of missingness. The resulting heatmap provides a visual representation of missing values in the dataset,\n",
        "                                                                      # making it easier to identify patterns of missingness across different columns."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2PWHfHZF8DwB"
      },
      "source": [
        "# crop.isnull().sum().sum()\n",
        "crop.isnull().sum().sum() gives you the total number of missing values in the entire DataFrame crop, summing up the counts of missing values across all columns. This information is valuable for understanding the extent of missing data in your dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hlV-6rBK8HlU"
      },
      "outputs": [],
      "source": [
        "crop.isnull().sum().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5gSN3EO076Zr"
      },
      "source": [
        "# crop.duplicated().sum()\n",
        "crop.duplicated().sum() gives you the total number of duplicated rows in the DataFrame crop. This information is useful for identifying and handling duplicate entries in your dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eVcc6nvA776x"
      },
      "outputs": [],
      "source": [
        "crop.duplicated().sum() # This line applies the duplicated() method to the DataFrame crop, which returns a boolean Series indicating whether each row is,\n",
        "                        # a duplicate of a previous row. The sum() method is then used to count the total number of True values in this Series,\n",
        "                        # indicating the total number of duplicate rows in the DataFrame. This calculation provides insight into the presence of duplicate data entries in the dataset,\n",
        "                        # which can affect analysis and should be handled appropriately, such as by removing duplicates to ensure data integrity."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sX0rzEI68YtD"
      },
      "source": [
        "# interpolation\n",
        "this code snippet ensures that missing values in the DataFrame crop are handled by interpolating them (linear interpolation) and by replacing missing values in the 'label' column with the most frequent label value. The resulting DataFrame crop1 has missing values appropriately handled."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a6TrzhvP8aAA"
      },
      "outputs": [],
      "source": [
        "# crop1 = crop.interpolate()\n",
        "# most_frequent_label = crop1['label'].mode()[0]\n",
        "# crop1['label'].fillna(most_frequent_label, inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QmeO55gMPgY9"
      },
      "source": [
        "# Handling Null values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8z5W65keKDcU"
      },
      "outputs": [],
      "source": [
        "# Atfirst we copy the data into crop1 variable.\n",
        "\n",
        "crop1 = crop  # This line creates a copy of the DataFrame crop and assigns it to a new variable crop1.\n",
        "              # This ensures that any modifications made to crop1 won't affect the original DataFrame crop.\n",
        "\n",
        "              # These lines fill missing values in specific columns of the DataFrame crop1 with predetermined values.\n",
        "              # Numeric columns are filled with 0, while the 'label' column is filled with the mode value.\n",
        "\n",
        "# Filling null values in 'Nitrogen' with 0\n",
        "crop1.N.fillna(0, inplace=True)\n",
        "\n",
        "# Filling null values in 'Phosphorus' with 0\n",
        "crop1.P.fillna(0, inplace=True)\n",
        "\n",
        "# Filling null values in 'Potassium' with 0\n",
        "crop1.K.fillna(0, inplace=True)\n",
        "\n",
        "# Filling null values in 'Temperature' with 0\n",
        "crop1.temperature.fillna(0, inplace=True)\n",
        "\n",
        "# Filling null values in 'Humidity' with 0\n",
        "crop1.humidity.fillna(0, inplace=True)\n",
        "\n",
        "# Filling null values in 'ph' with 0\n",
        "crop1.ph.fillna(0, inplace=True)\n",
        "\n",
        "# Filling null values in 'Rainfall' with 0\n",
        "crop1.rainfall.fillna(0, inplace=True)\n",
        "\n",
        "#Filling null values in 'Label' with 0\n",
        "crop1.label.fillna(crop1.label.mode()[0], inplace=True)\n",
        "\n",
        "\n",
        "# filling the null values with mean +- std in non-binary columns\n",
        "for column in ['N', 'P', 'K', 'temperature', 'humidity', 'ph', 'rainfall']:    # This loop fills missing values in numeric columns with the mean plus or minus the standard deviation.\n",
        "                                                                               # It calculates the mean and standard deviation of each column,\n",
        "                                                                               # then generates random values within a range centered around the mean,\n",
        "                                                                               # and finally assigns these random values to the missing entries in each column.\n",
        "    avg = crop1[column].mean()\n",
        "    std = crop1[column].std()\n",
        "    count = crop1[column].isnull().sum()\n",
        "    random = np.random.randint(avg-std,avg+std,size=count)\n",
        "    crop1[column][np.isnan(crop1[column])]=random\n",
        "\n",
        "# Check for null values\n",
        "sns.heatmap(crop1.isnull(),yticklabels=False,cbar=True,cmap='viridis')\n",
        "\n",
        "# Describe the data\n",
        "crop1.describe(include=\"all\")\n",
        "\n",
        "# Count: The total number of non-missing values in each column.\n",
        "# Unique: The number of unique categories in the column. This is relevant only for categorical data. NaN indicates that the column is numeric.\n",
        "# Top: The most frequently occurring category in the column. Again, only relevant for categorical data.\n",
        "# Freq: The frequency of the top category.\n",
        "# Mean: The average value of the column.\n",
        "# Std (Standard Deviation): Measures the amount of variation or dispersion of the data.\n",
        "# Min: The smallest value in the column.\n",
        "# 25% (First Quartile): The value below which 25% of the data falls.\n",
        "# 50% (Median): The middle value of the data.\n",
        "# 75% (Third Quartile): The value below which 75% of the data falls.\n",
        "# Max: The largest value in the column."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TmixFuFH8pwl"
      },
      "source": [
        "# Here we recheck if any null value is present or not."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yv1j65CN8sn8"
      },
      "outputs": [],
      "source": [
        "crop1.isnull().sum()  # This line applies the isnull() method to the DataFrame crop1, resulting in a DataFrame of the same shape with,\n",
        "                      # True for missing values and False for non-missing values. Then, the sum() method is used to calculate the sum of True values,\n",
        "                      #  (i.e., the number of missing values) for each column. The result is a Series where the index represents column names,\n",
        "                      # and the values represent the total number of missing values in each column.\n",
        "                      # This summary provides an overview of missing values in the dataset after the data preprocessing steps applied earlier."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xTmnK89sQ13p"
      },
      "source": [
        "# Here we recheck if any null value is present or not by heatmap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fA4rXmJPQ9yM"
      },
      "outputs": [],
      "source": [
        "sns.heatmap(crop.isnull(),yticklabels=False,cbar=True,cmap='viridis')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Check the Duplicate values\n",
        "\n"
      ],
      "metadata": {
        "id": "8mW2zAg21Uzq"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DUH0JQYm_Nio"
      },
      "source": [
        "# crop2 = crop1.drop_duplicates()\n",
        "the DataFrame crop2 contains the data from crop1 with duplicate rows removed. This operation ensures that each row in crop2 is unique, based on all columns by default."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AkMeuuvb_WYv"
      },
      "outputs": [],
      "source": [
        "crop2 = crop1.drop_duplicates()   # This line of code creates a new DataFrame named crop2 by applying the drop_duplicates() method to the DataFrame crop1.\n",
        "                                  # The drop_duplicates() method removes duplicate rows from the DataFrame based on all columns' values.\n",
        "                                  # After removing duplicates, the resulting DataFrame crop2 contains only unique rows, with duplicate rows eliminated.\n",
        "                                  # This operation ensures that each row in the DataFrame crop2 is unique, which can be useful for various analyses where,\n",
        "                                  # duplicate rows are not desired.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sns.heatmap(pd.DataFrame(crop2.duplicated()), yticklabels=False, cbar=True, cmap='viridis')"
      ],
      "metadata": {
        "id": "3c9Ur9nf5gM9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nIrsMP6K_nRA"
      },
      "source": [
        "# Here we recheck if any duplicate rows are present or not."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_MGdYIL3_oQH"
      },
      "outputs": [],
      "source": [
        " crop2.duplicated().sum()  # This line of code applies the duplicated() method to the DataFrame crop2.\n",
        "                           # The duplicated() method returns a boolean Series indicating whether each row is a duplicate of an earlier row or not.\n",
        "                           # True values indicate duplicate rows, while False values indicate unique rows.\n",
        "                           # The sum() method is then used to count the total number of True values in the Series,\n",
        "                           # representing the total number of duplicate rows in the DataFrame crop2.\n",
        "                           # This count provides information about the number of duplicate rows remaining in the DataFrame after removing duplicates,\n",
        "                           # with the drop_duplicates() method earlier. It helps to assess the effectiveness of the duplicate removal process.\n",
        "                           # If the count is zero, it indicates that no duplicate rows remain in the DataFrame crop2."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gxC7_K6yABFx"
      },
      "source": [
        "# crop2.isnull().any()\n",
        "returns a Series where each entry represents whether there are any missing values in the corresponding column of the DataFrame crop2. If the entry is True, it means that the column contains at least one missing value; otherwise, it's False."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8TtgDjqwAE1s"
      },
      "outputs": [],
      "source": [
        "crop2.isnull().any() # This line of code applies the isnull() method to the DataFrame crop2.\n",
        "                     # The isnull() method returns a DataFrame of the same shape as crop2, where each element is True if the corresponding element in crop2 is null (NaN), and False otherwise.\n",
        "                     # The any() method is then applied to the resulting DataFrame, which returns a boolean Series indicating whether there are any True values along each column axis (axis=0). True indicates the presence of at least one missing value (NaN) in the column, while False indicates that the column contains no missing values.\n",
        "                     # The output provides information about the presence of missing values in each column of the DataFrame crop2. If a column has True, it means there is at least one missing value in that column. Otherwise, the column contains no missing values."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## After remove the null values and duplicates values we check the shape of the data set"
      ],
      "metadata": {
        "id": "ti8SqdEcAifE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "crop2.shape\n"
      ],
      "metadata": {
        "id": "Og3M17-vAFq8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0bUY8scHBgfb"
      },
      "source": [
        "# crop2.describe()\n",
        "The output of crop2.describe() will be a DataFrame where each row represents a summary statistic, and each column represents a numerical column in the original DataFrame crop2. This summary statistics can provide insights into the distribution and spread of values in the dataset, helping with data exploration and analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gd_gpFwnBjl0"
      },
      "outputs": [],
      "source": [
        "crop2.describe() # This line of code calls the describe() method on the DataFrame crop2.\n",
        "                 # The describe() method computes summary statistics for each numerical column in the DataFrame, including count, mean, standard deviation, minimum, 25th percentile (first quartile), median (50th percentile),\n",
        "                 # 75th percentile (third quartile), and maximum.\n",
        "                 # For categorical columns, describe() provides count, unique, top (most frequent value), and freq (frequency of the top value).\n",
        "                 # This summary provides valuable insights into the central tendency, dispersion, and distribution of numerical data in the DataFrame crop2, as\n",
        "                 # well as the frequency and distribution of categorical data. It helps in understanding the overall characteristics of the dataset and identifying potential outliers or anomalies.\n",
        "\n",
        "                 # Describe Function In Python Pandas"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Count: The total number of non-missing values in each column.\n",
        "* Mean: The average value of the column.\n",
        "* Std (Standard Deviation): Measures the amount of variation or dispersion of the data.\n",
        "* Min: The smallest value in the column.\n",
        "* 25% (First Quartile): The value below which 25% of the data falls.\n",
        "* 50% (Median): The middle value of the data.\n",
        "* 75% (Third Quartile): The value below which 75% of the data falls.\n",
        "* Max: The largest value in the column."
      ],
      "metadata": {
        "id": "gKNp_mlT6ekl"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3bqh73fABn66"
      },
      "source": [
        "# corr()\n",
        "The corr() method is commonly used to identify relationships between variables in a dataset. High positive or negative correlation coefficients can indicate strong relationships between variables, while a correlation coefficient close to 0 suggests little to no relationship. These correlation coefficients can be further analyzed and interpreted to gain insights into the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f_YsTe6pBqis"
      },
      "outputs": [],
      "source": [
        "crop2_numeric = crop2.select_dtypes(include=[np.number])\n",
        "corr = crop2_numeric.corr()\n",
        "corr\n",
        "\n",
        "# The first line selects only the numeric columns from the DataFrame crop2 and stores them in a new DataFrame called crop2_numeric.\n",
        "# This is achieved using the select_dtypes() method with the include parameter set to [np.number], which selects columns with numeric data types.\n",
        "# The second line calculates the correlation matrix for the numeric columns in the DataFrame crop2_numeric.\n",
        "# The correlation matrix is a square matrix where each entry represents the correlation coefficient between two variables. It indicates the strength and direction of the linear relationship between pairs of variables.\n",
        "# The third line displays the correlation matrix, showing the correlation coefficients between all pairs of numeric columns in the DataFrame crop2_numeric.\n",
        "# Positive values indicate a positive correlation, negative values indicate a negative correlation, and values close to zero indicate little to no correlation.\n",
        "# This matrix helps in understanding the relationships between different variables in the dataset, which is valuable for feature selection, dimensionality reduction, and predictive modeling tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* This correlation matrix represents the correlation coefficients between pairs of numeric variables in the dataset.\n",
        "* The value of 1.000 along the diagonal represents the correlation of each variable with itself, which is always perfect (perfect positive correlation).\n",
        "* Off-diagonal values represent the correlation between pairs of variables. Each cell contains the correlation coefficient, which ranges from -1 to 1.\n",
        "* A value of 1 indicates a perfect positive correlation, meaning that as one variable increases, the other also increases proportionally.\n",
        "* A value of -1 indicates a perfect negative correlation, meaning that as one variable increases, the other decreases proportionally.\n",
        "* A value close to 0 indicates little to no linear correlation between the variables.\n",
        "* For example, the correlation coefficient between 'N' and 'P' is approximately -0.233, indicating a moderate negative correlation between these two variables.\n",
        "* Similarly, the correlation coefficient between 'K' and 'humidity' is approximately 0.182, indicating a moderate positive correlation between these two variables."
      ],
      "metadata": {
        "id": "jx1jriXI-O-H"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sH2YpNElF_mh"
      },
      "source": [
        "#Visualizing the correlation matrix\n",
        " By visualizing the correlation matrix as a heatmap, you can easily identify patterns and relationships between variables in the dataset. Positive correlations will appear in warm colors, negative correlations in cool colors, and no correlation in neutral colors. The annotations provide the exact correlation coefficients, making it easier to interpret the heatmap."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ca0NBElEGEPx"
      },
      "outputs": [],
      "source": [
        "sns.heatmap(corr,annot=True,cbar=True, cmap='coolwarm') # Generate a heatmap to visualize the correlation matrix 'corr' with annotations"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "**Comments**:\n",
        "- The `sns.heatmap()` function from the Seaborn library is used to create a heatmap.\n",
        "- The `corr` parameter specifies the correlation matrix that we want to visualize.\n",
        "- Setting `annot=True` adds annotations to each cell of the heatmap, displaying the correlation coefficients.\n",
        "- The `cbar=True` parameter adds a color bar to the side of the heatmap, indicating the scale of the colors.\n",
        "- The `cmap='coolwarm'` parameter sets the color map to 'coolwarm', which ranges from cool (blue) to warm (red), representing negative and positive correlations respectively.\n",
        "- This visualization helps in understanding the strength and direction of the linear relationships between pairs of variables in the dataset, making it easier to identify patterns and dependencies."
      ],
      "metadata": {
        "id": "LkLlJ6BcFnHi"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4PrVjR-77rrz"
      },
      "source": [
        "# size\n",
        "The .size method in Python is used to get the number of elements in an object, such as a list, tuple, set, or dictionary. However, it seems you've added parentheses to the method, which is not correct for this purpose."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BUkk_pNkBzYM"
      },
      "outputs": [],
      "source": [
        " crop2.size"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* This line of code calculates the total number of elements in the DataFrame crop2.\n",
        "* The size attribute of a DataFrame returns the total number of elements, which is equal to the number of rows multiplied by the number of columns.\n",
        "* The result represents the total number of data points or cells in the DataFrame crop2, including both numeric and categorical values."
      ],
      "metadata": {
        "id": "hC61eFIu_eCw"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oiy9RuMKC5NE"
      },
      "source": [
        "#columns\n",
        "The .columns attribute is used to retrieve the column labels of a DataFrame in pandas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v3Hlm0DBDBI9"
      },
      "outputs": [],
      "source": [
        " crop2.columns # Retrieve the column names of the DataFrame 'crop2'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* This line of code retrieves the column names of the DataFrame crop2.\n",
        "* The columns attribute of a DataFrame returns an Index object containing the column names.\n",
        "* The result represents the names of all columns in the DataFrame crop2."
      ],
      "metadata": {
        "id": "httNruCHBbxa"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kN1IYIncDqUq"
      },
      "source": [
        "#  crop2['label'].unique()\n",
        "The code crop2['label'].unique() is used to get the unique values in the 'label' column of the DataFrame crop2.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FAA81jy9EC47"
      },
      "outputs": [],
      "source": [
        "crop2['label'].unique() # Retrieve unique values from the 'label' column in the DataFrame 'crop2'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "**Comments**:\n",
        "- This line of code accesses the 'label' column of the DataFrame `crop2`.\n",
        "- The `.unique()` method is then applied to this column, which returns an array of unique values present in the 'label' column.\n",
        "- The result represents an array containing all unique values found in the 'label' column of the DataFrame `crop2`.\n",
        "- This operation is useful for understanding the different categories or classes present in the 'label' column, providing insights into the diversity of crops or labels present in the dataset."
      ],
      "metadata": {
        "id": "7ac6d3h3DWWr"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ik7UGgY8En87"
      },
      "source": [
        "#dtypes\n",
        "The dtypes attribute in pandas DataFrame is used to get the data types of each column in the DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xap2AJdLEs-V"
      },
      "outputs": [],
      "source": [
        " crop2.dtypes # Retrieve the data types of each column in the DataFrame 'crop2'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "crop2"
      ],
      "metadata": {
        "id": "6grVG4DepH7q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Comments**:\n",
        "- This line of code returns the data types of each column in the DataFrame `crop2`.\n",
        "- The `dtypes` attribute of a DataFrame provides information about the data type of each column.\n",
        "- The result is a Series where the index represents column names, and the values represent the corresponding data types of each column in `crop2`.\n",
        "- Understanding the data types is important for data manipulation and analysis, as it informs about the nature of the data stored in each column (e.g., integer, float, object, etc.), helping in appropriate data handling and processing."
      ],
      "metadata": {
        "id": "IvRNhtIZEA_u"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pBV3YwuxFPbN"
      },
      "source": [
        "# crop2['label'].value_counts()\n",
        "\n",
        "* The output of crop2['label'].value_counts() will be a Series where each unique label in the 'label' column of crop2 is listed along with the count of occurrences of that label in the dataset. This information is useful for understanding the distribution of different labels in the dataset and can be valuable for various analytical purposes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kX--oO5fFR-j"
      },
      "outputs": [],
      "source": [
        "crop2['label'].value_counts() # Count the occurrences of each unique value in the 'label' column of the DataFrame 'crop2'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "**Comments**:\n",
        "- This line of code accesses the 'label' column of the DataFrame `crop2`.\n",
        "- The `.value_counts()` method is then applied to this column, which returns a Series containing the count of each unique value in the 'label' column.\n",
        "- The index of the resulting Series represents the unique values found in the 'label' column, and the corresponding values represent the frequency of each unique value.\n",
        "- This operation is useful for understanding the distribution of different categories or classes in the 'label' column, providing insights into the frequency of occurrence of each crop label in the dataset."
      ],
      "metadata": {
        "id": "OOV2aYroE1jg"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rEKJ_LckU2ew"
      },
      "source": [
        "#Distribution of values in the 'N' column of the DataFrame crop2.\n",
        "\n",
        "By executing this code, you'll generate a distribution plot showing the distribution of values in the 'N' column of the DataFrame crop2. This visualization helps in understanding the distribution of values and identifying any patterns or outliers present in the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "49_cOctOGcjs"
      },
      "outputs": [],
      "source": [
        "sns.distplot(crop2['N'])  # Generate a distribution plot (histogram and kernel density estimation) for the 'N' column in DataFrame 'crop2'\n",
        "\n",
        "plt.show() # Display the plot\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "**Comments**:\n",
        "- The `sns.distplot()` function from the Seaborn library is used to create a distribution plot.\n",
        "- The column 'N' from the DataFrame `crop2` is passed as the data to be plotted.\n",
        "- This plot combines a histogram and a kernel density estimation (KDE) plot to visualize the distribution of values in the 'N' column.\n",
        "- The `plt.show()` function is used to display the plot.\n",
        "- This visualization provides insights into the distribution of values in the 'N' column, including measures of central tendency and spread."
      ],
      "metadata": {
        "id": "lkFj03udG2ON"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K8syEzXyVoPQ"
      },
      "source": [
        "#Distribution of values in the 'P' column of the DataFrame crop2.\n",
        "\n",
        "By executing this code, you'll generate a distribution plot showing the distribution of values in the 'P' column of the DataFrame crop2. This visualization helps in understanding the distribution of values and identifying any patterns or outliers present in the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gmK_QOhmGnLL"
      },
      "outputs": [],
      "source": [
        "sns.distplot(crop2['P'])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MVxV-tq_Vx1d"
      },
      "source": [
        "#Distribution plot of K, Temperature, Humidity, ph, Rainfall using Seaborn's distplot() function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FyCGZBetWT6p"
      },
      "outputs": [],
      "source": [
        "sns.distplot(crop2['K'])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eiyOsalPWakn"
      },
      "outputs": [],
      "source": [
        "sns.distplot(crop2['temperature'])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DGsE3fWrWmGZ"
      },
      "outputs": [],
      "source": [
        "sns.distplot(crop2['ph'])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DjsZGvjGWrQ4"
      },
      "outputs": [],
      "source": [
        "sns.distplot(crop2['rainfall'])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EocCEb3lfmU8"
      },
      "source": [
        "#Checking the outliers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sns.distplot(crop2['N'])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "GfPQAytcTlpY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.distplot(crop2['P'])\n"
      ],
      "metadata": {
        "id": "pZc0Qj7CT78e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f, axes = plt.subplots(7, 2, figsize=(10, 40))\n",
        "sns.boxplot(y=\"N\", data=crop1, palette=\"winter\", ax=axes[0][0]).set_title(\"Before\")\n",
        "plt.grid()\n",
        "\n",
        "sns.boxplot(y=\"P\", data=crop1, palette=\"winter\", ax=axes[1][0]).set_title(\"Before\")\n",
        "plt.grid()\n",
        "\n",
        "sns.boxplot(y=\"K\", data=crop1, palette=\"winter\", ax=axes[2][0]).set_title(\"Before\")\n",
        "plt.grid()\n",
        "\n",
        "sns.boxplot(y=\"temperature\", data=crop1, palette=\"winter\", ax=axes[3][0]).set_title(\"Before\")\n",
        "plt.grid()\n",
        "\n",
        "sns.boxplot(y=\"humidity\", data=crop1, palette=\"winter\", ax=axes[4][0]).set_title(\"Before\")\n",
        "plt.grid()\n",
        "\n",
        "sns.boxplot(y=\"ph\", data=crop1, palette=\"winter\", ax=axes[5][0]).set_title(\"Before\")\n",
        "plt.grid()\n",
        "\n"
      ],
      "metadata": {
        "id": "5Ikgy4StrDVj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Check and clean the outliers of P"
      ],
      "metadata": {
        "id": "OCG3RzXOFSK-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.boxplot(x=crop2['P'])"
      ],
      "metadata": {
        "id": "2NR3CupxUEd8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "crop3 = crop2.copy()\n",
        "crop3\n",
        "len(crop3)"
      ],
      "metadata": {
        "id": "cZ7sQKjmobB9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Handel the outliers using IQR method"
      ],
      "metadata": {
        "id": "G0-Ejk75F2xe"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WYr9yJ2Dpm_U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Calculate Q1, Q3, and IQR for 'P'\n",
        "Q1 = crop3['P'].quantile(0.25)\n",
        "Q3 = crop3['P'].quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "\n",
        "print('Q1=',Q1,', Q3=', Q3,', IQR=' ,IQR)\n",
        "\n",
        "# Step 2: Define limits for what is considered an outlier\n",
        "\n",
        "upper_limit = Q3 + (1.5 * IQR)\n",
        "lower_limit = Q1 - (1.5 * IQR)\n",
        "\n",
        "print(\"lower_limit=\",lower_limit)\n",
        "print(\"upper_limit\",upper_limit)\n",
        "\n",
        "#Find the outliers\n",
        "# print(\"The outliers of P\")\n",
        "crop3.loc[(crop3['P'] > upper_limit) | (crop3['P'] < lower_limit)]\n",
        "\n",
        "#trimming the data\n",
        "new_df = crop3.loc[(crop3['P'] < upper_limit) & (crop3['P'] > lower_limit)]\n",
        "print(\"before removing outliers:\",len(crop3))\n",
        "print(\"after removing outliers:\",len(new_df))\n",
        "print(\"outliers:\",len(crop3)-len(new_df))"
      ],
      "metadata": {
        "id": "Fc61W_m3clCA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.boxplot(x=new_df['P'])"
      ],
      "metadata": {
        "id": "28p0Ko_qx7kh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Capping\n",
        "new_df = crop3.copy()\n",
        "new_df.loc[(new_df['P'] > upper_limit), 'P'] = upper_limit\n",
        "new_df.loc[(new_df['P'] < lower_limit), 'P'] = lower_limit"
      ],
      "metadata": {
        "id": "SKuUUjoSyMWE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.boxplot(x=new_df['P'])"
      ],
      "metadata": {
        "id": "QOD2jetsymsX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Check and clean the outliers of K"
      ],
      "metadata": {
        "id": "BgwMcfvAFdHV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.boxplot(x=crop2['K'])"
      ],
      "metadata": {
        "id": "LtQI8F2SFmLU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Calculate Q1, Q3, and IQR for 'P'\n",
        "Q1 = crop3['K'].quantile(0.25)\n",
        "Q3 = crop3['K'].quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "\n",
        "print('Q1=',Q1,', Q3=', Q3,', IQR=' ,IQR)\n",
        "\n",
        "# Step 2: Define limits for what is considered an outlier\n",
        "\n",
        "upper_limit = Q3 + (1.5 * IQR)\n",
        "lower_limit = Q1 - (1.5 * IQR)\n",
        "\n",
        "print(\"lower_limit=\",lower_limit)\n",
        "print(\"upper_limit\",upper_limit)\n",
        "\n",
        "#Find the outliers\n",
        "# print(\"The outliers of P\")\n",
        "crop3.loc[(crop3['K'] > upper_limit) | (crop3['K'] < lower_limit)]\n",
        "\n",
        "#trimming the data\n",
        "new_df = crop3.loc[(crop3['K'] < upper_limit) & (crop3['K'] > lower_limit)]\n",
        "print(\"before removing outliers:\",len(crop3))\n",
        "print(\"after removing outliers:\",len(new_df))\n",
        "print(\"outliers:\",len(crop3)-len(new_df))"
      ],
      "metadata": {
        "id": "DGNZ3l2OF_xR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.boxplot(x=new_df['K'])"
      ],
      "metadata": {
        "id": "kvErb5xLzufQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Capping\n",
        "new_df = crop3.copy()\n",
        "new_df.loc[(new_df['K'] > upper_limit), 'K'] = upper_limit\n",
        "new_df.loc[(new_df['K'] < lower_limit), 'K'] = lower_limit"
      ],
      "metadata": {
        "id": "AZoiBxcMz46K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.boxplot(x=new_df['K'])"
      ],
      "metadata": {
        "id": "hh5jq-Z_0CQa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Check and clean the outliers of temperature"
      ],
      "metadata": {
        "id": "PMJ6vO_JI0Xv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.boxplot(x=crop2['temperature'])"
      ],
      "metadata": {
        "id": "A_OrgFgUJeTg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Calculate Q1, Q3, and IQR for 'P'\n",
        "Q1 = crop3['temperature'].quantile(0.25)\n",
        "Q3 = crop3['temperature'].quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "\n",
        "print('Q1=',Q1,', Q3=', Q3,', IQR=' ,IQR)\n",
        "\n",
        "# Step 2: Define limits for what is considered an outlier\n",
        "\n",
        "upper_limit = Q3 + (1.5 * IQR)\n",
        "lower_limit = Q1 - (1.5 * IQR)\n",
        "\n",
        "print(\"lower_limit=\",lower_limit)\n",
        "print(\"upper_limit\",upper_limit)\n",
        "\n",
        "#Find the outliers\n",
        "# print(\"The outliers of P\")\n",
        "crop3.loc[(crop3['temperature'] > upper_limit) | (crop3['temperature'] < lower_limit)]\n",
        "\n",
        "#trimming the data\n",
        "new_df = crop3.loc[(crop3['temperature'] < upper_limit) & (crop3['temperature'] > lower_limit)]\n",
        "print(\"before removing outliers:\",len(crop3))\n",
        "print(\"after removing outliers:\",len(new_df))\n",
        "print(\"outliers:\",len(crop3)-len(new_df))"
      ],
      "metadata": {
        "id": "a-s5KwIEJ63b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.boxplot(x=new_df['temperature'])"
      ],
      "metadata": {
        "id": "MB5M0p0P1D7S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Capping\n",
        "new_df = crop3.copy()\n",
        "new_df.loc[(new_df['temperature'] > upper_limit), 'temperature'] = upper_limit\n",
        "new_df.loc[(new_df['temperature'] < lower_limit), 'temperature'] = lower_limit"
      ],
      "metadata": {
        "id": "9D2RtqYQ1MYc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.boxplot(x=new_df['temperature'])"
      ],
      "metadata": {
        "id": "tlSJzr221cTJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Check and clean the outliers of humidity"
      ],
      "metadata": {
        "id": "f6NMwse-Ko3p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.boxplot(x=crop3['humidity'])"
      ],
      "metadata": {
        "id": "-_xVAQnGKxzg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Calculate Q1, Q3, and IQR for 'P'\n",
        "Q1 = crop3['humidity'].quantile(0.25)\n",
        "Q3 = crop3['humidity'].quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "\n",
        "print('Q1=',Q1,', Q3=', Q3,', IQR=' ,IQR)\n",
        "\n",
        "# Step 2: Define limits for what is considered an outlier\n",
        "\n",
        "upper_limit = Q3 + (1.5 * IQR)\n",
        "lower_limit = Q1 - (1.5 * IQR)\n",
        "\n",
        "print(\"lower_limit=\",lower_limit)\n",
        "print(\"upper_limit\",upper_limit)\n",
        "\n",
        "#Find the outliers\n",
        "# print(\"The outliers of P\")\n",
        "crop3.loc[(crop3['humidity'] > upper_limit) | (crop3['humidity'] < lower_limit)]\n",
        "\n",
        "#trimming the data\n",
        "new_df = crop3.loc[(crop3['humidity'] < upper_limit) & (crop3['humidity'] > lower_limit)]\n",
        "print(\"before removing outliers:\",len(crop3))\n",
        "print(\"after removing outliers:\",len(new_df))\n",
        "print(\"outliers:\",len(crop3)-len(new_df))"
      ],
      "metadata": {
        "id": "5IDx96heLKxR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.boxplot(x=new_df['humidity'])"
      ],
      "metadata": {
        "id": "-RLJZbkC2JTF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Capping\n",
        "new_df = crop3.copy()\n",
        "new_df.loc[(new_df['humidity'] > upper_limit), 'humidity'] = upper_limit\n",
        "new_df.loc[(new_df['humidity'] < lower_limit), 'humidity'] = lower_limit"
      ],
      "metadata": {
        "id": "dD19JHaM2SQf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.boxplot(x=new_df['humidity'])"
      ],
      "metadata": {
        "id": "QEf7uobb2f9h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Check and clean the outliers of ph"
      ],
      "metadata": {
        "id": "uLsVs6bULr2w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.boxplot(x=crop3['ph'])"
      ],
      "metadata": {
        "id": "VM1R83RnLylv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Calculate Q1, Q3, and IQR for 'ph'\n",
        "Q1 = crop3['ph'].quantile(0.25)\n",
        "Q3 = crop3['ph'].quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "\n",
        "print('Q1=',Q1,', Q3=', Q3,', IQR=' ,IQR)\n",
        "\n",
        "# Step 2: Define limits for what is considered an outlier\n",
        "\n",
        "upper_limit = Q3 + (1.5 * IQR)\n",
        "lower_limit = Q1 - (1.5 * IQR)\n",
        "\n",
        "print(\"lower_limit=\",lower_limit)\n",
        "print(\"upper_limit\",upper_limit)\n",
        "\n",
        "#Find the outliers\n",
        "# print(\"The outliers of P\")\n",
        "crop3.loc[(crop3['ph'] > upper_limit) | (crop3['ph'] < lower_limit)]\n",
        "\n",
        "#trimming the data\n",
        "new_df = crop3.loc[(crop3['ph'] < upper_limit) & (crop3['ph'] > lower_limit)]\n",
        "print(\"before removing outliers:\",len(crop3))\n",
        "print(\"after removing outliers:\",len(new_df))\n",
        "print(\"outliers:\",len(crop3)-len(new_df))"
      ],
      "metadata": {
        "id": "Luy76yD6L6yB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.boxplot(x=new_df['ph'])"
      ],
      "metadata": {
        "id": "nLZivBD03TJc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Capping\n",
        "new_df = crop3.copy()\n",
        "new_df.loc[(new_df['ph'] > upper_limit), 'ph'] = upper_limit\n",
        "new_df.loc[(new_df['ph'] < lower_limit), 'ph'] = lower_limit"
      ],
      "metadata": {
        "id": "1bLcgEuV3iqc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.boxplot(x=new_df['ph'])"
      ],
      "metadata": {
        "id": "QINBHM4f3snb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Check and clean the outliers of rainfall"
      ],
      "metadata": {
        "id": "El1BtsjoOKSd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.boxplot(x=crop3['rainfall'])"
      ],
      "metadata": {
        "id": "cY2XX-knOSyJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Calculate Q1, Q3, and IQR for 'rainfall'\n",
        "Q1 = crop3['rainfall'].quantile(0.25)\n",
        "Q3 = crop3['rainfall'].quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "\n",
        "print('Q1=',Q1,', Q3=', Q3,', IQR=' ,IQR)\n",
        "\n",
        "# Step 2: Define limits for what is considered an outlier\n",
        "\n",
        "upper_limit = Q3 + (1.5 * IQR)\n",
        "lower_limit = Q1 - (1.5 * IQR)\n",
        "\n",
        "print(\"lower_limit=\",lower_limit)\n",
        "print(\"upper_limit\",upper_limit)\n",
        "\n",
        "#Find the outliers\n",
        "# print(\"The outliers of P\")\n",
        "crop3.loc[(crop3['rainfall'] > upper_limit) | (crop3['rainfall'] < lower_limit)]\n",
        "\n",
        "#trimming the data\n",
        "new_df = crop3.loc[(crop3['rainfall'] < upper_limit) & (crop3['rainfall'] > lower_limit)]\n",
        "print(\"before removing outliers:\",len(crop3))\n",
        "print(\"after removing outliers:\",len(new_df))\n",
        "print(\"outliers:\",len(crop3)-len(new_df))"
      ],
      "metadata": {
        "id": "y7KtdszNOZsq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.boxplot(x=new_df['rainfall'])"
      ],
      "metadata": {
        "id": "oPYD9W1V4ejt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Capping\n",
        "new_df = crop3.copy()\n",
        "new_df.loc[(new_df['rainfall'] > upper_limit), 'rainfall'] = upper_limit\n",
        "new_df.loc[(new_df['rainfall'] < lower_limit), 'rainfall'] = lower_limit"
      ],
      "metadata": {
        "id": "OUbLu8jx4nB7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.boxplot(x=new_df['rainfall'])"
      ],
      "metadata": {
        "id": "qvCQuKvB4w1d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rku1jYjzD-UI"
      },
      "source": [
        "# Visualising other attributes with respect to our target column i.e. 'Label'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7qwDHgy_EBL5"
      },
      "outputs": [],
      "source": [
        "f, axes = plt.subplots(7, 1, figsize=(10, 40))\n",
        "\n",
        "sns.barplot(x=new_df.N, y=crop2.label, ax=axes.flat[0])\n",
        "\n",
        "sns.barplot(x=new_df.P, y=crop2.label, ax=axes.flat[1])\n",
        "\n",
        "sns.barplot(x=new_df.K, y=crop2.label, ax=axes.flat[2])\n",
        "\n",
        "sns.barplot(x=new_df.temperature, y=crop2.label, ax=axes.flat[3])\n",
        "\n",
        "sns.barplot(x=new_df.ph, y=crop2.label, ax=axes.flat[4])\n",
        "\n",
        "sns.barplot(x=new_df.humidity, y=crop2.label, ax=axes.flat[5])\n",
        "\n",
        "sns.barplot(x=new_df.rainfall, y=crop2.label, ax=axes.flat[6])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KnTt79rIAlJk"
      },
      "source": [
        "# Seperating features and target label\n",
        " this code prepares your data by selecting relevant features and the target variable for use in a machine learning model. The features are the input variables used to make predictions, while the target variable is the output variable you want to predict."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w7zDRPZYAlJm"
      },
      "outputs": [],
      "source": [
        "features =  new_df[['N', 'P','K','temperature', 'humidity', 'ph', 'rainfall']]\n",
        "target =   new_df['label']\n",
        "#features = df[['temperature', 'humidity', 'ph', 'rainfall']]\n",
        "labels =   new_df['label']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1gXwhnjdRAti"
      },
      "source": [
        "# Initialzing empty lists to append all model's name and corresponding name\n",
        "By initializing these lists, you're setting up a structure to collect and organize the results of model evaluation, likely for comparison or further analysis. As you evaluate different models, you can append their accuracy scores to the acc list and their names to the model list, allowing you to track and analyze the performance of each model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hha-78JvAlJm"
      },
      "outputs": [],
      "source": [
        "# Initialzing empty lists to append all model's name and corresponding name\n",
        "acc = []\n",
        "model = []"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lVbcQJQgSv1X"
      },
      "source": [
        "### The line **from sklearn.model_selection import train_test_split** imports the train_test_split function from the sklearn.model_selection module. This function is commonly used in machine learning to split datasets into training and testing sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AFxxMk4VS7gN"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h6GYytGwMgd2"
      },
      "source": [
        "### The line **Xtrain, Xtest, Ytrain, Ytest = train_test_split(features,target,test_size = 0.2,random_state =2)** uses the train_test_split function from scikit-learn to split the dataset into training and testing sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JuGLc9VYT6p0"
      },
      "outputs": [],
      "source": [
        "Xtrain, Xtest, Ytrain, Ytest = train_test_split(features,target,test_size = 0.25,random_state =2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AbKfyjzmmfKI"
      },
      "source": [
        "# Now we will train our model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XMcpgpb1m35i"
      },
      "source": [
        "In machine learning (ML), model training involves the process of feeding data into a machine learning algorithm or model to enable it to learn patterns, relationships, and insights from the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "slZyW1JZn2ap"
      },
      "source": [
        "##Decision Tree\n",
        "Decision tree training involves recursively partitioning the input space (feature space) into smaller regions while aiming to minimize impurity or maximize information gain at each step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yzqFG7ULpOux"
      },
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "DecisionTree = DecisionTreeClassifier(criterion=\"entropy\", random_state=2, max_depth=5)\n",
        "\n",
        "DecisionTree.fit(Xtrain, Ytrain)\n",
        "predicted_values_train = DecisionTree.predict(Xtrain)\n",
        "\n",
        "training_accuracy = accuracy_score(Ytrain, predicted_values_train)\n",
        "print(\"Training Accuracy:\", training_accuracy)\n",
        "\n",
        "predicted_values_test = DecisionTree.predict(Xtest)\n",
        "testing_accuracy = accuracy_score(Ytest, predicted_values_test)\n",
        "acc.append(testing_accuracy)\n",
        "model.append('Decision Tree')\n",
        "print(\"Decision Tree's Testing Accuracy:\", testing_accuracy)\n",
        "\n",
        "print(\"Classification Report for Testing Data:\")\n",
        "print(classification_report(Ytest, predicted_values_test))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G6EqPsQNjYC0"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "cm = confusion_matrix(Ytest, predicted_values_test)\n",
        "plt.figure(figsize=(10,10))\n",
        "sns.heatmap(cm, annot=True, fmt=\".0f\", linewidths=.5, square = True, cmap = 'Blues');\n",
        "plt.ylabel('Actual label');\n",
        "plt.xlabel('Predicted label');\n",
        "all_sample_title = 'Confusion Matrix - score:'+str(accuracy_score(Ytest,predicted_values_test))\n",
        "plt.title(all_sample_title, size = 15);\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2LXERLlStpLf"
      },
      "source": [
        "# from sklearn.model_selection import cross_val_score\n",
        "\n",
        "This line of code imports the cross_val_score function from the model_selection module of scikit-learn.cross_val_score is used for cross-validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eKrPmejgt2vC"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import cross_val_score\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yHUL7-_hvRgN"
      },
      "source": [
        "# Cross validation score (Decision Tree)\n",
        "After executing this line of code, score will contain an array of cross-validated scores, allowing you to assess the performance of your decision tree model across multiple train-test splits of the data. These scores can then be used to estimate the model's generalization ability and variability."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "meE7pEU9vSdI"
      },
      "outputs": [],
      "source": [
        "score = cross_val_score(DecisionTree, features, target,cv=5)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zxmJGPrtwT7p"
      },
      "source": [
        "#score\n",
        "The variable score contains the cross-validated scores obtained from the cross_val_score function. These scores represent the performance of the decision tree model (DecisionTree) across different folds of the cross-validation process"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uqXZp3A0wYqp"
      },
      "outputs": [],
      "source": [
        "score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FQj1I1Cvpfdv"
      },
      "outputs": [],
      "source": [
        "\n",
        "print(\"Mean CV score: {0:.2f}\".format(score.mean()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cOZERh-TAlJo"
      },
      "source": [
        "### Saving trained Decision Tree model\n",
        "To save a trained Decision Tree classifier using the pickle module in Python."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qr6C10LUwp_W"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "# Dump the trained Naive Bayes classifier with Pickle\n",
        "DT_pkl_filename = '/content/gdrive/My Drive/models/DecisionTree.pkl'\n",
        "# Open the file to save as pkl file\n",
        "DT_Model_pkl = open(DT_pkl_filename, 'wb')\n",
        "pickle.dump(DecisionTree, DT_Model_pkl)\n",
        "# Close the pickle instances\n",
        "DT_Model_pkl.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EmEzbdb_AlJo"
      },
      "source": [
        "# Guassian Naive Bayes\n",
        "Gaussian Naive Bayes (GNB) is a variant of the Naive Bayes algorithm that assumes the features are continuous and follows a Gaussian (normal) distribution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RErh7aSD6lA3"
      },
      "outputs": [],
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "NaiveBayes = GaussianNB()\n",
        "\n",
        "NaiveBayes.fit(Xtrain, Ytrain)\n",
        "\n",
        "predicted_values_train = NaiveBayes.predict(Xtrain)\n",
        "\n",
        "training_accuracy = accuracy_score(Ytrain, predicted_values_train)\n",
        "print(\"Training Accuracy:\", training_accuracy)\n",
        "\n",
        "predicted_values_test = NaiveBayes.predict(Xtest)\n",
        "testing_accuracy = accuracy_score(Ytest, predicted_values_test)\n",
        "acc.append(testing_accuracy)\n",
        "model.append('Naive Bayes')\n",
        "print(\"Naive Bayes's Testing Accuracy:\", testing_accuracy)\n",
        "\n",
        "print(\"Classification Report for Testing Data:\")\n",
        "print(classification_report(Ytest, predicted_values_test))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_bmPvD6kk5EE"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "cm = confusion_matrix(Ytest, predicted_values_test)\n",
        "plt.figure(figsize=(10,10))\n",
        "sns.heatmap(cm, annot=True, fmt=\".0f\", linewidths=.5, square = True, cmap = 'Blues');\n",
        "plt.ylabel('Actual label');\n",
        "plt.xlabel('Predicted label');\n",
        "all_sample_title = 'Confusion Matrix - score:'+str(accuracy_score(Ytest,predicted_values_test))\n",
        "plt.title(all_sample_title, size = 15);\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LGroJDaC7PVn"
      },
      "source": [
        "# Cross validation score (NaiveBayes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L9fRX-Gn7QMP"
      },
      "outputs": [],
      "source": [
        "score = cross_val_score(NaiveBayes,features,target,cv=5)\n",
        "score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DHwtsYX_zboD"
      },
      "outputs": [],
      "source": [
        "print(\"Mean CV score: {0:.2f}\".format(score.mean()))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vorriCMEAlJo"
      },
      "source": [
        "### Saving trained Guassian Naive Bayes model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AsM_xr0i786S"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "# Dump the trained Naive Bayes classifier with Pickle\n",
        "NB_pkl_filename = '/content/gdrive/My Drive/models//NBClassifier.pkl'\n",
        "# Open the file to save as pkl file\n",
        "NB_Model_pkl = open(NB_pkl_filename, 'wb')\n",
        "pickle.dump(NaiveBayes, NB_Model_pkl)\n",
        "# Close the pickle instances\n",
        "NB_Model_pkl.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AUKypYeWAlJp"
      },
      "source": [
        "# Support Vector Machine (SVM)\n",
        "Support Vector Machine or SVM is one of the most popular Supervised Learning algorithms, which is used for Classification as well as Regression problems. However, primarily, it is used for Classification problems in Machine Learning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "61FBv2QZ9DYC"
      },
      "outputs": [],
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Fit scaler on training data\n",
        "norm = MinMaxScaler().fit(Xtrain)\n",
        "\n",
        "# Transform training and testing data\n",
        "X_train_norm = norm.transform(Xtrain)\n",
        "X_test_norm = norm.transform(Xtest)\n",
        "\n",
        "# Initialize and train the SVM model\n",
        "SVM = SVC(kernel='poly', degree=3, C=1)\n",
        "SVM.fit(X_train_norm, Ytrain)\n",
        "\n",
        "# Predictions and evaluation on training data\n",
        "predicted_values_train = SVM.predict(X_train_norm)\n",
        "training_accuracy = accuracy_score(Ytrain, predicted_values_train)\n",
        "print(\"Training Accuracy:\", training_accuracy)\n",
        "\n",
        "# Predictions and evaluation on testing data\n",
        "predicted_values_test = SVM.predict(X_test_norm)\n",
        "testing_accuracy = accuracy_score(Ytest, predicted_values_test)\n",
        "# print(\"Testing Accuracy:\", testing_accuracy)\n",
        "\n",
        "# Append accuracy and model name to respective lists\n",
        "acc.append(testing_accuracy)\n",
        "model.append('SVM')\n",
        "\n",
        "# Print testing accuracy and classification report\n",
        "print(\"SVM's Testing Accuracy:\", testing_accuracy)\n",
        "print(\"Classification Report for Testing Data:\")\n",
        "print(classification_report(Ytest, predicted_values_test))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G6kPzs4NlOo3"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "cm = confusion_matrix(Ytest, predicted_values_test)\n",
        "plt.figure(figsize=(10,10))\n",
        "sns.heatmap(cm, annot=True, fmt=\".0f\", linewidths=.5, square = True, cmap = 'Blues');\n",
        "plt.ylabel('Actual label');\n",
        "plt.xlabel('Predicted label');\n",
        "all_sample_title = 'Confusion Matrix - score:'+str(accuracy_score(Ytest,predicted_values_test))\n",
        "plt.title(all_sample_title, size = 15);\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I3q-bvyjHsXB"
      },
      "source": [
        "# Cross validation score (SVM)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pjDyn3XiHor8"
      },
      "outputs": [],
      "source": [
        "# Cross validation score (SVM)\n",
        "score = cross_val_score(SVM,features,target,cv=5)\n",
        "score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yg0-x_eF1bVj"
      },
      "outputs": [],
      "source": [
        "print(\"Mean CV score: {0:.2f}\".format(score.mean()))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B38hPPRo1S6g"
      },
      "outputs": [],
      "source": [
        "testing_accuracy = accuracy_score(Ytest, predicted_values_test)\n",
        "print(\"SVM's Testing Accuracy:\",testing_accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "imniWayuIX6U"
      },
      "source": [
        "#Saving trained SVM model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ngy518HbIZ6k"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "# Dump the trained SVM classifier with Pickle\n",
        "SVM_pkl_filename = '/content/gdrive/My Drive/models/SVMClassifier.pkl'\n",
        "# Open the file to save as pkl file\n",
        "SVM_Model_pkl = open(SVM_pkl_filename, 'wb')\n",
        "pickle.dump(SVM, SVM_Model_pkl)\n",
        "# Close the pickle instances\n",
        "SVM_Model_pkl.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6an70inMAlJq"
      },
      "source": [
        "# Logistic Regression\n",
        "Logistic regression is used for binary classification where we use sigmoid function, that takes input as independent variables and produces a probability value between 0 and 1.\n",
        "\n",
        "For example, we have two classes Class 0 and Class 1 if the value of the logistic function for an input is greater than 0.5 (threshold value) then it belongs to Class 1 it belongs to Class 0. It’s referred to as regression because it is the extension of linear regression but is mainly used for classification problems."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K_IQaUZeJrUB"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "LogReg = LogisticRegression(random_state=2)\n",
        "\n",
        "LogReg.fit(Xtrain, Ytrain)\n",
        "\n",
        "predicted_values_train = LogReg.predict(Xtrain)\n",
        "\n",
        "training_accuracy = accuracy_score(Ytrain, predicted_values_train)\n",
        "print(\"Training Accuracy:\", training_accuracy)\n",
        "\n",
        "predicted_values_test = LogReg.predict(Xtest)\n",
        "testing_accuracy = accuracy_score(Ytest, predicted_values_test)\n",
        "acc.append(testing_accuracy)\n",
        "model.append('Logistic Regression')\n",
        "print(\"Logistic Regression's Testing Accuracy:\", testing_accuracy)\n",
        "\n",
        "print(\"Classification Report for Testing Data:\")\n",
        "print(classification_report(Ytest, predicted_values_test))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mhYxWr2_lday"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "cm = confusion_matrix(Ytest, predicted_values_test)\n",
        "plt.figure(figsize=(10,10))\n",
        "sns.heatmap(cm, annot=True, fmt=\".0f\", linewidths=.5, square = True, cmap = 'Blues');\n",
        "plt.ylabel('Actual label');\n",
        "plt.xlabel('Predicted label');\n",
        "all_sample_title = 'Confusion Matrix - score:'+str(accuracy_score(Ytest,predicted_values_test))\n",
        "plt.title(all_sample_title, size = 15);\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ti5GUB8bLRnK"
      },
      "source": [
        "# Cross validation score (Logistic Regression)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sLJMB-krLSRW"
      },
      "outputs": [],
      "source": [
        "score = cross_val_score(LogReg,features,target,cv=5)\n",
        "score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xs4Gschj3ooc"
      },
      "outputs": [],
      "source": [
        "print(\"Mean CV score:\", score.mean())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OtP54A2TAlJr"
      },
      "source": [
        "### Saving trained Logistic Regression model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C6tYNHDdMtQ-"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "# Dump the trained Naive Bayes classifier with Pickle\n",
        "LR_pkl_filename = '/content/gdrive/My Drive/models/LogisticRegression.pkl'\n",
        "# Open the file to save as pkl file\n",
        "LR_Model_pkl = open(LR_pkl_filename, 'wb')\n",
        "pickle.dump(LogReg, LR_Model_pkl)\n",
        "# Close the pickle instances\n",
        "LR_Model_pkl.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OY1MZgAiAlJr"
      },
      "source": [
        "# Random Forest\n",
        "Random Forest is a popular machine learning algorithm that belongs to the supervised learning technique. It can be used for both Classification and Regression problems in ML. It is based on the concept of ensemble learning, which is a process of combining multiple classifiers to solve a complex problem and to improve the performance of the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ov-Mrb_ANvfd"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "RF = RandomForestClassifier(n_estimators=20, random_state=0)\n",
        "RF.fit(Xtrain, Ytrain)\n",
        "\n",
        "predicted_values_train = RF.predict(Xtrain)\n",
        "\n",
        "training_accuracy = accuracy_score(Ytrain, predicted_values_train)\n",
        "print(\"Training Accuracy:\", training_accuracy)\n",
        "\n",
        "predicted_values_test = RF.predict(Xtest)\n",
        "testing_accuracy = accuracy_score(Ytest, predicted_values_test)\n",
        "acc.append(testing_accuracy)\n",
        "model.append('RF')\n",
        "print(\"RF's Testing Accuracy:\", testing_accuracy)\n",
        "\n",
        "print(\"Classification Report for Testing Data:\")\n",
        "print(classification_report(Ytest, predicted_values_test))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qGa83MIol9DU"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "cm = confusion_matrix(Ytest, predicted_values_test)\n",
        "plt.figure(figsize=(10,10))\n",
        "sns.heatmap(cm, annot=True, fmt=\".0f\", linewidths=.5, square = True, cmap = 'Blues');\n",
        "plt.ylabel('Actual label');\n",
        "plt.xlabel('Predicted label');\n",
        "all_sample_title = 'Confusion Matrix - score:'+str(accuracy_score(Ytest, predicted_values_test))\n",
        "plt.title(all_sample_title, size = 15);\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2brg43VeOfyz"
      },
      "source": [
        "# Cross validation score (Random Forest)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "niSdLMu2OhEr"
      },
      "outputs": [],
      "source": [
        "score = cross_val_score(RF,features,target,cv=5)\n",
        "score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qlC5qwWX4NdB"
      },
      "outputs": [],
      "source": [
        "print(\"Mean CV score:\", score.mean())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Us1LDTEoAlJs"
      },
      "source": [
        "### Saving trained Random Forest model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YiIb5QqZPrj1"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "# Dump the trained Naive Bayes classifier with Pickle\n",
        "RF_pkl_filename = '/content/gdrive/My Drive/models/RandomForest.pkl'\n",
        "# Open the file to save as pkl file\n",
        "RF_Model_pkl = open(RF_pkl_filename, 'wb')\n",
        "pickle.dump(RF, RF_Model_pkl)\n",
        "# Close the pickle instances\n",
        "RF_Model_pkl.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w87c4DR8AlJt"
      },
      "source": [
        "## Accuracy Comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z-v49O4pQBot"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=[10,5],dpi = 100)\n",
        "plt.title('Accuracy Comparison')\n",
        "plt.xlabel('Accuracy')\n",
        "plt.ylabel('Algorithm')\n",
        "sns.barplot(x = acc,y = model,palette='dark')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "28ab0CqYRBeY"
      },
      "outputs": [],
      "source": [
        "accuracy_models = dict(zip(model, acc))\n",
        "for k, v in accuracy_models.items():\n",
        "    print (k, '-->', v)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TRS806tEAlJu"
      },
      "source": [
        "## Making a prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "njComSreRvDT"
      },
      "outputs": [],
      "source": [
        "data = np.array([[101,87,54,29,76,6.3,100]])\n",
        "prediction = RF.predict(data)[0]\n",
        "print(\"{} is a best crop to be cultivated. \".format(prediction))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CAQUfDkTR769"
      },
      "outputs": [],
      "source": [
        "def recommendation(N,P,k,temperature,humidity,ph,rainfal):\n",
        "    features = np.array([[N,P,k,temperature,humidity,ph,rainfal]])\n",
        "    prediction = RF.predict(features)[0]\n",
        "\n",
        "    return prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oSS1QjwySmSM"
      },
      "outputs": [],
      "source": [
        "N = 2\n",
        "P = 123\n",
        "k = 198\n",
        "temperature = 39.64\n",
        "humidity = 82.21\n",
        "ph = 6.25\n",
        "rainfall = 70.39\n",
        "predict = recommendation(N,P,k,temperature,humidity,ph,rainfall)\n",
        "print(\"{} is a best crop to be cultivated. \".format(predict))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}